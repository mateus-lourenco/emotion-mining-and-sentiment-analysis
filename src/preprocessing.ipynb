{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1648597741011,
     "user": {
      "displayName": "Mateus Cunha Lourenço",
      "userId": "17013560479942289923"
     },
     "user_tz": 180
    },
    "id": "qgfIb_44n_iY",
    "outputId": "6ef20146-e3e1-4e4f-81bf-5c437b872819"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Não Se Esquecerá</td>\n",
       "      <td>Forrozão Tropykália</td>\n",
       "      <td>0</td>\n",
       "      <td>Sei que poderá levar um tempo Sei que poderá d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title               artist  year  \\\n",
       "0  Não Se Esquecerá  Forrozão Tropykália     0   \n",
       "\n",
       "                                               lyric  \n",
       "0  Sei que poderá levar um tempo Sei que poderá d...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/lyrics.csv', sep=';', encoding=\"utf-8\")\n",
    "\n",
    "# Remoção de colunas que não serão utilizadas na análise\n",
    "df.drop(columns=['id', 'composer', 'album'], inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza dos títulos das músicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'De Coração Virado (Morena)'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_title = df.loc[87, 'title']\n",
    "normal_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "pLJkEckLGFs5"
   },
   "outputs": [],
   "source": [
    "# igualando todos os títulos para minúsculo\n",
    "df['title'] = df['title'].str.lower()\n",
    "\n",
    "# limpeza dos títulos com símbolos\n",
    "df['title'].replace(\"\\((.*?)\\)\", '', regex=True, inplace=True)\n",
    "\n",
    "# removendo espaços nas extremidades\n",
    "df['title'] = df['title'].str.strip()\n",
    "\n",
    "#capitalizando título\n",
    "df['title'] = df['title'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'De Coração Virado'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_title = df.loc[87, 'title']\n",
    "processed_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OjwT4QIP6DD"
   },
   "source": [
    "### Remoção das músicas sem data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1648597745452,
     "user": {
      "displayName": "Mateus Cunha Lourenço",
      "userId": "17013560479942289923"
     },
     "user_tz": 180
    },
    "id": "2kAqfTutOJ3y",
    "outputId": "48d2a812-dc73-43a4-ca84-7022fe5d3a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de músicas com data: 11046\n"
     ]
    }
   ],
   "source": [
    "df = df[df['year'] > 0]\n",
    "\n",
    "print(f'Quantidade de músicas com data: {df.title.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxMvcbz1IepS"
   },
   "source": [
    "### Remoção dos registros sem letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11046 entries, 2359 to 13404\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   11046 non-null  object\n",
      " 1   artist  11046 non-null  object\n",
      " 2   year    11046 non-null  int64 \n",
      " 3   lyric   10995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 431.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1648598631168,
     "user": {
      "displayName": "Mateus Cunha Lourenço",
      "userId": "17013560479942289923"
     },
     "user_tz": 180
    },
    "id": "tD2STUlLHriL",
    "outputId": "33677e2c-dcbc-4fc9-80ba-53e1eb38ccef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de registros com letras: 10995\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['lyric'].isnull()]\n",
    "print(f'Quantidade de registros com letras: {df.lyric.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10995 entries, 2359 to 13404\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   10995 non-null  object\n",
      " 1   artist  10995 non-null  object\n",
      " 2   year    10995 non-null  int64 \n",
      " 3   lyric   10995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 429.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processamento das letras musicais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./data/clean_lyrics.csv', index=False)\n",
    "df = pd.read_csv('./data/clean_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando dataset de sujeiras encontradas em algumas letras\n",
    "noise = pd.read_csv('./data/stop_words.txt')\n",
    "noise = noise['stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "lyric_noise = set(noise.to_list())\n",
    "stop_words = STOP_WORDS.union(lyric_noise)\n",
    "stop_words = [word for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_lyrics(text):\n",
    "  txt = ''\n",
    "  #caracteres em minusculo   \n",
    "  txt = text.lower()\n",
    "  #coletando apenas letras\n",
    "  words = re.findall('[a-zéóáêâãõç|A-ZÉÓÁÊÂÃÕÇ]+', txt)\n",
    "  txt = (' ').join(words)\n",
    "  #remove stop words\n",
    "  doc = nlp(txt)\n",
    "  txt = ' '.join( token.text \n",
    "                  for token in doc \n",
    "                  if not token.is_digit\n",
    "                  and token.text not in stop_words\n",
    "                )\n",
    "  \n",
    "  return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes: Eu fui dançar Twist lá pra banda do Caju Mas lá pra meia-noite começou um sururu (2x) Iê, iê, iê, iê, iê, que confusão Iê, iê, iê, só sendo a tentação do cão (coro repete) Tudo aconteceu naquela brincadeira, Porque o dono do baile barrou uma feiticeira Que saiu se maldizendo daquela festança E entrou no cemitério pedindo vingança Iê, iê, iê, iê, iê, que confusão Iê, iê, iê, só sendo a tentação do cão (coro repete) Quando eu vi a coisa preta, tratei de correr Mas uma voz diferente não parava de dizer: \"Se correr o bicho pega, se ficar o bicho come\" Olhei pra minha esquerda, vi um lobisomem Iê, iê, iê, iê, iê, que confusão Iê, iê, iê, só sendo a tentação do cão (coro repete)\n",
      "depois: Eu dançar Twist banda Caju Mas meia noite começou sururu Iê confusão Iê sendo tentação cão coro Tudo aconteceu brincadeira Porque dono baile barrou feiticeira Que saiu maldizendo festança E entrou cemitério pedindo vingança Iê confusão Iê sendo tentação cão coro Quando vi preta tratei correr Mas diferente parava Se correr bicho pega ficar bicho come Olhei esquerda vi lobisomem Iê confusão Iê sendo tentação cão coro\n"
     ]
    }
   ],
   "source": [
    "## Testando preprocessmaneto\n",
    "normal_lyric = df.loc[6, 'lyric']\n",
    "processed_lyric = clean_lyrics(normal_lyric)\n",
    "\n",
    "print(f'antes: {normal_lyric}')\n",
    "print(f'depois: {processed_lyric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removendo ruídos das letras\n",
    "df['clean_lyric'] = df['lyric'].apply(clean_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bertaglia, Thales Felipe Costa, and Maria das Graças Volpe Nunes. \n",
    "“Exploring Word Embeddings for Unsupervised Textual User-Generated Content Normalization.” \n",
    "Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT). 2016.\n",
    "\"\"\"\n",
    "from enelvo.normaliser import Normaliser\n",
    "norm = Normaliser()\n",
    "\n",
    "def clean_lyrics_2(lyric):\n",
    "    doc = nlp(lyric)\n",
    "    vocab = []\n",
    "    for token in doc:\n",
    "        if token.lower_ not in stop_words:\n",
    "            if token.pos_ == 'VERB':\n",
    "                vocab.append(token.lemma)\n",
    "            elif token.pos_ == 'PROPN':\n",
    "                vocab.append(token.text)\n",
    "            else:\n",
    "                vocab.append(token.lower_)\n",
    "\n",
    "    normalized = ' '.join(norm.normalise(word) for word in vocab)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes: Tenho Não discuto teimoso Não perder O precioso É viver Tenho João Teimoso nome Dorme pé rede Dá bebida fome Dá comida sede Foge meninas boas Diz prefere coroas Quando começa pára Este cara cismo cara\n"
     ]
    }
   ],
   "source": [
    "processed_lyric = df.loc[0, 'clean_lyric']\n",
    "print(f'antes: {processed_lyric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lyric'] = df['clean_lyric'].apply(clean_lyrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot tokenize non-string, 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdepois: \u001b[39m\u001b[39m{\u001b[39;00mclean_lyrics_2(processed_lyric)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb Cell 23'\u001b[0m in \u001b[0;36mclean_lyrics_2\u001b[0;34m(lyric)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=17'>18</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=18'>19</a>\u001b[0m             vocab\u001b[39m.\u001b[39mappend(token\u001b[39m.\u001b[39mlower_)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=20'>21</a>\u001b[0m normalized \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(norm\u001b[39m.\u001b[39;49mnormalise(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m vocab)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normalized\n",
      "\u001b[1;32m/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb Cell 23'\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=17'>18</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=18'>19</a>\u001b[0m             vocab\u001b[39m.\u001b[39mappend(token\u001b[39m.\u001b[39mlower_)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=20'>21</a>\u001b[0m normalized \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(norm\u001b[39m.\u001b[39;49mnormalise(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocab)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/digivox/development/emotion-mining-and-sentiment-analysis/src/preprocessing.ipynb#ch0000022vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normalized\n",
      "File \u001b[0;32m~/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py:121\u001b[0m, in \u001b[0;36mNormaliser.normalise\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalise\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=113'>114</a>\u001b[0m     \u001b[39m\"\"\"Normalises a given sentence and returns it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=114'>115</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=115'>116</a>\u001b[0m \u001b[39m        sentece (str): The sentence to be normalised.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=118'>119</a>\u001b[0m \u001b[39m        str: Normalised sentence.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=119'>120</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=120'>121</a>\u001b[0m     pp_line \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39;49mtokenize(text\u001b[39m=\u001b[39;49msentence, tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer)\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=121'>122</a>\u001b[0m     oov_tokens \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=122'>123</a>\u001b[0m         analytics\u001b[39m.\u001b[39midentify_oov(\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=123'>124</a>\u001b[0m             lex\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mok_lex, force_list\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_list, tokens\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpp_line\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=126'>127</a>\u001b[0m         \u001b[39melse\u001b[39;00m analytics\u001b[39m.\u001b[39midentify_oov(lex\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mok_lex, tokens\u001b[39m=\u001b[39mpp_line)\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=127'>128</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=128'>129</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m oov_tokens:\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/normaliser.py?line=129'>130</a>\u001b[0m         \u001b[39m# In case force list contains forced corrections\u001b[39;00m\n",
      "File \u001b[0;32m~/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/preprocessing.py:43\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(text, tokenizer, as_string)\u001b[0m\n\u001b[1;32m     <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/preprocessing.py?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/preprocessing.py?line=41'>42</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m Tokenizer()\n\u001b[0;32m---> <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/preprocessing.py?line=42'>43</a>\u001b[0m tokenized \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n\u001b[1;32m     <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/preprocessing.py?line=43'>44</a>\u001b[0m tokenized \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokenized]\n\u001b[1;32m     <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/preprocessing.py?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m as_string \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokenized)\n",
      "File \u001b[0;32m~/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py:382\u001b[0m, in \u001b[0;36mTokenizer.tokenize\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=370'>371</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=371'>372</a>\u001b[0m \u001b[39mTokenize the given string into a list of strings representing the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=372'>373</a>\u001b[0m \u001b[39mconstituent words of the message.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=378'>379</a>\u001b[0m \u001b[39m@param message: The string representation of the message.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=379'>380</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=380'>381</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(message, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=381'>382</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=382'>383</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcannot tokenize non-string, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mrepr\u001b[39m(\u001b[39mtype\u001b[39m(message)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=383'>384</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=384'>385</a>\u001b[0m message \u001b[39m=\u001b[39m _converthtmlentities(_unicode(message))\n\u001b[1;32m    <a href='file:///home/digivox/development/emotion-mining-and-sentiment-analysis/.nlp/lib/python3.8/site-packages/enelvo/preprocessing/tokenizer/tokenizer.py?line=385'>386</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignorequotes:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot tokenize non-string, 'int'"
     ]
    }
   ],
   "source": [
    "print(f'depois: {clean_lyrics_2(processed_lyric)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para remover palavras que contêm repetições sequenciais\n",
    "def remove_words_with_reps(text):\n",
    "  words = text.split()\n",
    "  for i, word in enumerate(words):\n",
    "      reps = 0\n",
    "      for j, character in enumerate(word):\n",
    "        if j > 0 and character == word[j-1]:\n",
    "            reps+=1\n",
    "      if reps > 1:\n",
    "        words.pop(i)\n",
    "      \n",
    "  return (' ').join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes: diferença mulher home esperar home cabelo peitar queixo cabeludo mulher não so dia manhã adão comer maçã eva comeu ficar adão eva adão dar mancada eva dar mulher perna braço coxa nariz boca muita inteligência bicho homem jeito reparar direito pouquinho diferença breque achar diferença sapato não homem\n",
      "depois: diferença mulher home esperar home cabelo peitar queixo cabeludo mulher não so dia manhã adão comer maçã eva comeu ficar adão eva adão dar mancada eva dar mulher perna braço coxa nariz boca muita inteligência bicho homem jeito reparar direito pouquinho diferença breque achar diferença sapato não homem\n"
     ]
    }
   ],
   "source": [
    "#Testando removedor de sequenciais\n",
    "lyric = df.loc[245, 'clean_lyric']\n",
    "processed_lyric = remove_words_with_reps(lyric)\n",
    "\n",
    "print(f'antes: {lyric}')\n",
    "print(f'depois: {processed_lyric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lyric'] = df['clean_lyric'].apply(remove_words_with_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### criando decadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para criar decadas\n",
    "decade = lambda year: int(((((year - (year % 10)) / 10) % 10) * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerando decadas\n",
    "df['decade'] = df['year'].apply(decade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gerando colunas de palavras únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para retornar palavras unicas\n",
    "unique = lambda text: set(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique_words'] = df['clean_lyric'].apply(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passando letras para listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para retornar palavras em formato de lista\n",
    "to_words = lambda text: text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando letras em lista de palavras\n",
    "df['words'] = df['clean_lyric'].apply(to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#armazenando\n",
    "df.to_csv('./data/clean_lyrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNQR2xgyW7WOrCQnX18yeFp",
   "collapsed_sections": [],
   "name": "higienizacao_df.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "5529f27cbc555093496ea2b5ca5b0c995313c8d0bdabb96b48cdf9d2b85f39ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "22b04671355daadbbed3e848c51a8cb4ceea07cad10492c09cb5f8a9e64564b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
